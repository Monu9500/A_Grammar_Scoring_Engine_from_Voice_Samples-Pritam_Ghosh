{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a079ae",
   "metadata": {},
   "source": [
    "## build an AI Grammar Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047b127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Whisper AI Model (Speech-to-Text)...\n",
      "‚è≥ Loading Grammar Checker Tool...\n",
      "‚úÖ SUCCESS: All libraries and models are loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import language_tool_python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP PATHS\n",
    "# ---------------------------------------------------------\n",
    "# I point to the \"dataset\" folder relative to this notebook\n",
    "BASE_DIR = \"dataset\" \n",
    "\n",
    "# Define exact paths to your folders based on your structure\n",
    "TRAIN_AUDIO_DIR = os.path.join(BASE_DIR, \"audios\", \"train\")\n",
    "TEST_AUDIO_DIR = os.path.join(BASE_DIR, \"audios\", \"test\")\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, \"csvs\", \"train.csv\")\n",
    "TEST_CSV_PATH = os.path.join(BASE_DIR, \"csvs\", \"test.csv\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. LOAD MODELS\n",
    "# ---------------------------------------------------------\n",
    "print(\"‚è≥ Loading Whisper AI Model (Speech-to-Text)...\")\n",
    "# 'base' model is a good balance of speed and accuracy\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "print(\"‚è≥ Loading Grammar Checker Tool...\")\n",
    "# This tool checks for sentence structure and syntax errors\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "print(\"‚úÖ SUCCESS: All libraries and models are loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99b7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_grammar(filename, folder_path):\n",
    "    \"\"\"\n",
    "    1. Fixes the filename (adds .wav if missing).\n",
    "    2. Transcribes the audio to text using Whisper.\n",
    "    3. Counts grammar mistakes using LanguageTool.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- STEP 1: Fix Missing .wav Extension ---\n",
    "    # My CSV has \"audio_141\", but file is \"audio_141.wav\"\n",
    "    filename_str = str(filename)\n",
    "    if not filename_str.endswith('.wav'):\n",
    "        filename_str = filename_str + \".wav\"\n",
    "        \n",
    "    # Create the full path to the file\n",
    "    full_path = os.path.join(folder_path, filename_str)\n",
    "    \n",
    "    # Safety Check: Does the file actually exist?\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"‚ö†Ô∏è WARNING: Could not find file {full_path}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # --- STEP 2: Transcribe Audio (Speech -> Text) ---\n",
    "        # The AI listens to the file here\n",
    "        audio_result = whisper_model.transcribe(full_path)\n",
    "        text_content = audio_result['text']\n",
    "        \n",
    "        # --- STEP 3: Check Grammar ---\n",
    "        # The tool reads the text and finds errors\n",
    "        mistakes = tool.check(text_content)\n",
    "        error_count = len(mistakes)\n",
    "        word_count = len(text_content.split())\n",
    "        \n",
    "        # Avoid crashing if the file is silent (0 words)\n",
    "        if word_count == 0: word_count = 1\n",
    "            \n",
    "        return error_count, word_count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR processing {filename}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faaed4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 409 training samples in train.csv\n",
      "üöÄ Starting to process training files... Please wait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prita\\anaconda3\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 / 409 files...\n",
      "Processed 20 / 409 files...\n",
      "Processed 40 / 409 files...\n",
      "Processed 60 / 409 files...\n",
      "Processed 80 / 409 files...\n",
      "Processed 100 / 409 files...\n",
      "Processed 120 / 409 files...\n",
      "Processed 140 / 409 files...\n",
      "Processed 160 / 409 files...\n",
      "Processed 180 / 409 files...\n",
      "Processed 200 / 409 files...\n",
      "Processed 220 / 409 files...\n",
      "Processed 240 / 409 files...\n",
      "Processed 260 / 409 files...\n",
      "Processed 280 / 409 files...\n",
      "Processed 300 / 409 files...\n",
      "Processed 320 / 409 files...\n",
      "Processed 340 / 409 files...\n",
      "Processed 360 / 409 files...\n",
      "Processed 380 / 409 files...\n",
      "Processed 400 / 409 files...\n",
      "‚úÖ DONE: Training data processing is complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>error_density</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio_173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio_138</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audio_127</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio_95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio_73</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename  error_density  score\n",
       "0  audio_173       0.000000    3.0\n",
       "1  audio_138       0.013072    3.0\n",
       "2  audio_127       0.067797    2.0\n",
       "3   audio_95       0.000000    2.0\n",
       "4   audio_73       0.016393    3.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Read the Training CSV file\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "print(f\"üìÇ Found {len(train_df)} training samples in train.csv\")\n",
    "\n",
    "collected_data = []\n",
    "\n",
    "print(\"üöÄ Starting to process training files... Please wait.\")\n",
    "\n",
    "# 2. Loop through every file in the CSV\n",
    "for index, row in train_df.iterrows():\n",
    "    f_name = row['filename']\n",
    "    actual_score = row['label']\n",
    "    \n",
    "    # Call our function from Cell 2\n",
    "    errors, words = analyze_grammar(f_name, TRAIN_AUDIO_DIR)\n",
    "    \n",
    "    if errors is not None:\n",
    "        # Calculate \"Mistakes per Word\" (Error Density)\n",
    "        # Logic: More mistakes per word = Lower Grammar Score\n",
    "        density = errors / words\n",
    "        \n",
    "        collected_data.append({\n",
    "            'filename': f_name,\n",
    "            'error_density': density,\n",
    "            'score': actual_score\n",
    "        })\n",
    "    \n",
    "    # Print progress every 20 files\n",
    "    if index % 20 == 0:\n",
    "        print(f\"Processed {index} / {len(train_df)} files...\")\n",
    "\n",
    "# 3. Save extracted data into a table\n",
    "train_features = pd.DataFrame(collected_data)\n",
    "print(\"‚úÖ DONE: Training data processing is complete!\")\n",
    "# Show the first 5 rows of what we learned\n",
    "display(train_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a170ba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üìä REPORT FOR SUBMISSION:\n",
      "--------------------------------------------------\n",
      "Model Used: Linear Regression\n",
      "Feature Used: Error Density (Grammar Mistakes / Word Count)\n",
      "RMSE Score (Training Data): 0.76794\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Prepare Data for AI ---\n",
    "# X = The input (Mistakes per word)\n",
    "# y = The target (The grammar score 1-5)\n",
    "X = train_features[['error_density']]\n",
    "y = train_features['score']\n",
    "\n",
    "# --- STEP 2: Split Data ---\n",
    "# We keep 20% of data aside to test if our model is smart\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- STEP 3: Train the Model ---\n",
    "# We use Linear Regression (Simple and effective for this)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- STEP 4: Calculate Accuracy (RMSE) ---\n",
    "predictions = model.predict(X_val)\n",
    "rmse_score = np.sqrt(mean_squared_error(y_val, predictions))\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"üìä REPORT FOR SUBMISSION:\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(f\"Model Used: Linear Regression\")\n",
    "print(f\"Feature Used: Error Density (Grammar Mistakes / Word Count)\")\n",
    "print(f\"RMSE Score (Training Data): {rmse_score:.5f}\") \n",
    "print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b4aa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 197 test files to predict.\n",
      "üöÄ Starting to process TEST files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prita\\anaconda3\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 / 197...\n",
      "Processed 20 / 197...\n",
      "Processed 40 / 197...\n",
      "Processed 60 / 197...\n",
      "Processed 80 / 197...\n",
      "Processed 100 / 197...\n",
      "Processed 120 / 197...\n",
      "Processed 140 / 197...\n",
      "Processed 160 / 197...\n",
      "Processed 180 / 197...\n",
      "‚úÖ SUCCESS! 'submission.csv' has been created in your project folder.\n"
     ]
    }
   ],
   "source": [
    "# 1. Read the Test CSV file\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "print(f\"üìÇ Found {len(test_df)} test files to predict.\")\n",
    "\n",
    "test_data = []\n",
    "\n",
    "print(\"üöÄ Starting to process TEST files...\")\n",
    "\n",
    "# 2. Loop through every file in Test CSV\n",
    "for index, row in test_df.iterrows():\n",
    "    f_name = row['filename']\n",
    "    \n",
    "    # Note: We look in TEST_AUDIO_DIR this time\n",
    "    errors, words = analyze_grammar(f_name, TEST_AUDIO_DIR)\n",
    "    \n",
    "    if errors is not None:\n",
    "        density = errors / words\n",
    "        test_data.append({\n",
    "            'filename': f_name,\n",
    "            'error_density': density\n",
    "        })\n",
    "        \n",
    "    if index % 20 == 0:\n",
    "        print(f\"Processed {index} / {len(test_df)}...\")\n",
    "\n",
    "# 3. Convert to DataFrame\n",
    "test_features_df = pd.DataFrame(test_data)\n",
    "\n",
    "# 4. Predict the scores using our trained model\n",
    "predicted_scores = model.predict(test_features_df[['error_density']])\n",
    "\n",
    "# 5. Create the Submission DataFrame (filename, label)\n",
    "submission = pd.DataFrame({\n",
    "    'filename': test_df['filename'], # Use original filenames from CSV\n",
    "    'label': predicted_scores\n",
    "})\n",
    "\n",
    "# 6. Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"‚úÖ SUCCESS! 'submission.csv' has been created in your project folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c46b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
